---
description: when wrtiing code related to ollama api for llm streams and getting models from local ollama api 
globs: 
alwaysApply: false
---
## Endpoints

### Generate a Chat Completion

```
POST /api/chat
```

Generate the next message in a chat with a provided model.

#### Parameters

* `model`: name of the model to use
* `messages`: list of messages in the chat
* `tools`: list of tools to use in the chat (optional)
* `stream`: whether to stream the response (default: true)
* `keep_alive`: how long to keep the model in memory (default: 5m)

#### Examples

##### Request

```shell
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    }
  ]
}'
```

##### Response

A stream of JSON objects is returned:

```json
{
  "model": "llama3.2",
  "created_at": "2023-08-04T08:52:19.385406455-07:00",
  "message": {
    "role": "assistant",
    "content": "The"
  },
  "done": false
}
```

### List Local Models

```
GET /api/tags
```

List models that are available locally.

#### Examples

##### Request

```shell
curl http://localhost:11434/api/tags
```

##### Response

A single JSON object will be returned:

```json
{
  "models": [
    {
      "name": "codellama:13b",
      "modified_at": "2023-11-04T14:56:49.277302595-07:00",
      "size": 7365960935,
      "digest": "9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697",
      "details": {
        "format": "gguf",
        "family": "llama",
        "families": null,
        "parameter_size": "13B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

### Show Model Information

```
POST /api/show
```

Show information about a model.

#### Parameters

* `model`: name of the model to show

#### Examples

##### Request

```shell
curl http://localhost:11434/api/show -d '{
  "model": "llama3.2"
}'
```

##### Response

```json
{
  "modelfile": "...",
  "parameters": "...",
  "template": "...",
  "details": {
    "parent_model": "",
    "format": "gguf",
    "family": "llama",
    "families": [
      "llama"
    ],
    "parameter_size": "8.0B",
    "quantization_level": "Q4_0"
  }
}
```

### Create a Model

```
POST /api/create
```

Create a new model from an existing model.

#### Parameters

* `model`: name of the new model
* `from`: name of the existing model to create from
* `system`: system prompt for the new model (optional)

#### Examples

##### Request

```shell
curl http://localhost:11434/api/create -d '{
  "model": "mario",
  "from": "llama3.2",
  "system": "You are Mario from Super Mario Bros."
}'
```

##### Response

A stream of JSON objects is returned:

```json
{"status":"reading model metadata"}
{"status":"creating system layer"}
...
{"status":"success"}
```